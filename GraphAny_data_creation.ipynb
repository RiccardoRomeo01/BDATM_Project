{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# we install all the required for RelBench\n",
        "!pip install relbench[full]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1x7YyiiaBqq",
        "outputId": "e97692d7-2820-4de9-fbc3-5ab750baa4b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: relbench[full] in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from relbench[full]) (2.2.2)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.11/dist-packages (from relbench[full]) (1.8.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from relbench[full]) (17.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from relbench[full]) (1.26.4)\n",
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.11/dist-packages (from relbench[full]) (1.1.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from relbench[full]) (1.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from relbench[full]) (4.12.2)\n",
            "Requirement already satisfied: pytorch_frame>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from relbench[full]) (0.2.4)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (from relbench[full]) (2.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pytorch_frame>=0.2.3->relbench[full]) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pytorch_frame>=0.2.3->relbench[full]) (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from pytorch_frame>=0.2.3->relbench[full]) (11.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->relbench[full]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->relbench[full]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->relbench[full]) (2024.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch->relbench[full]) (4.3.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pooch->relbench[full]) (24.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch->relbench[full]) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->relbench[full]) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->relbench[full]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->relbench[full]) (3.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric->relbench[full]) (3.11.11)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric->relbench[full]) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric->relbench[full]) (3.1.5)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric->relbench[full]) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric->relbench[full]) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->relbench[full]) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch->relbench[full]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch->relbench[full]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch->relbench[full]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch->relbench[full]) (2024.12.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->relbench[full]) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->relbench[full]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->relbench[full]) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->relbench[full]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->relbench[full]) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->relbench[full]) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->relbench[full]) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric->relbench[full]) (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_frame>=0.2.3->relbench[full]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_frame>=0.2.3->relbench[full]) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_frame>=0.2.3->relbench[full]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_frame>=0.2.3->relbench[full]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_frame>=0.2.3->relbench[full]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_frame>=0.2.3->relbench[full]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_frame>=0.2.3->relbench[full]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_frame>=0.2.3->relbench[full]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_frame>=0.2.3->relbench[full]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_frame>=0.2.3->relbench[full]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_frame>=0.2.3->relbench[full]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_frame>=0.2.3->relbench[full]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_frame>=0.2.3->relbench[full]) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_frame>=0.2.3->relbench[full]) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_frame>=0.2.3->relbench[full]) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->pytorch_frame>=0.2.3->relbench[full]) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pytorch_frame>=0.2.3->relbench[full]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import relbench\n",
        "from relbench.datasets import get_dataset_names, get_dataset\n",
        "from relbench.modeling.utils import get_stype_proposal\n",
        "from relbench.modeling.graph import make_pkey_fkey_graph\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch_geometric.seed import seed_everything\n",
        "from torch import Tensor\n",
        "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
        "\n",
        "\n",
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "import pickle\n",
        "\n",
        "import requests"
      ],
      "metadata": {
        "id": "RicEzpD4aCYw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GloveTextEmbedding:\n",
        "    def __init__(self, device: Optional[torch.device] = None):\n",
        "        self.model = SentenceTransformer(\n",
        "            \"sentence-transformers/average_word_embeddings_glove.6B.300d\",\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "    def __call__(self, sentences: List[str]) -> Tensor:\n",
        "        return self.model.encode(sentences, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "DcGvYOdJaEUm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Checking"
      ],
      "metadata": {
        "id": "Qb4w685WaTM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that it's cuda if you want it to run in reasonable time!\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_num_threads(1)\n",
        "print(device)\n",
        "\n",
        "# Set the seed for generating random numbers to ensure reproducibility\n",
        "seed_everything(42)\n",
        "\n",
        "# Path to the directory for caching graph data\n",
        "root_dir = \"./data\"\n",
        "\n",
        "# Configure the text encoder\n",
        "text_embedder_cfg = TextEmbedderConfig(\n",
        "    text_embedder=GloveTextEmbedding(device=device),\n",
        "    batch_size=256\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omB338KcaG0N",
        "outputId": "0138fda7-8688-4e47-effb-e5e8f8584d88"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The RelBench version is {relbench.__version__}\")\n",
        "print(f\"The RelBench datasets are {get_dataset_names()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TBcZtIyaNET",
        "outputId": "1ef8f646-572f-411c-ecf3-539543cca24b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The RelBench version is 1.1.0\n",
            "The RelBench datasets are ['rel-amazon', 'rel-avito', 'rel-event', 'rel-f1', 'rel-hm', 'rel-stack', 'rel-trial']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Usefull functions"
      ],
      "metadata": {
        "id": "CLdeP7JQlhzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modify_node_name(key):\n",
        "    # Trova l'ultima posizione di '_' e rimuove la parte numerica\n",
        "    last_underscore_index = key.rfind('_')\n",
        "\n",
        "    if last_underscore_index != -1:\n",
        "        # Parte letterale (es. 'standings')\n",
        "        modified_name = key[:last_underscore_index]\n",
        "\n",
        "        # Parte numerica (es. 3)\n",
        "        node_index = int(''.join(filter(str.isdigit, key[last_underscore_index:])))\n",
        "\n",
        "        return modified_name, node_index\n",
        "\n",
        "    else:\n",
        "        node_name = ''.join(filter(str.isalpha, key))  # Parte letterale (es. 'standings')\n",
        "        node_index = int(''.join(filter(str.isdigit, key)))  # Parte numerica (es. 5)\n",
        "        return node_name, node_index"
      ],
      "metadata": {
        "id": "12-8o2dudjiR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_triplets_from_file(file_path):\n",
        "    triplets = []\n",
        "\n",
        "    # Controlla se il file_path è un URL\n",
        "    if file_path.startswith(\"http://\") or file_path.startswith(\"https://\"):\n",
        "        response = requests.get(file_path)\n",
        "        if response.status_code == 200:\n",
        "            lines = response.text.splitlines()\n",
        "        else:\n",
        "            print(f\"Errore nel download del file: {response.status_code}\")\n",
        "            return triplets\n",
        "    else:\n",
        "        # Legge il file locale\n",
        "        with open(file_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        # Rimuovi eventuali spazi bianchi e separa la riga in base ai tab\n",
        "        parts = line.strip().split('\\t')\n",
        "        if len(parts) == 3:  # Assicurati che ci siano esattamente 3 elementi\n",
        "            triplet = (parts[0], parts[1], parts[2])\n",
        "            triplets.append(triplet)\n",
        "        else:\n",
        "            print(f\"Riga non valida: {line.strip()}\")\n",
        "\n",
        "    return triplets\n"
      ],
      "metadata": {
        "id": "Hr4hygt8eUPM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_nodes_dictionary_from_triplets(node_dict, triplets, nodes_without_timestamp, split):\n",
        "\n",
        "    for triplet in triplets:\n",
        "      source_node = triplet[0]\n",
        "      edge_label = triplet[1]\n",
        "      target_node = triplet[2]\n",
        "\n",
        "      # se il source node e il target node non sono già nel vocabolario li aggiungo\n",
        "      if source_node not in node_dict:\n",
        "          source_node_label = modify_node_name(source_node)\n",
        "\n",
        "          if source_node_label in nodes_without_timestamp:\n",
        "            node_dict[source_node] = [len(node_dict), 'all']\n",
        "          else:\n",
        "            node_dict[source_node] = [len(node_dict), split]\n",
        "\n",
        "      if target_node not in node_dict:\n",
        "          target_node_label = modify_node_name(target_node)\n",
        "\n",
        "          if target_node_label in nodes_without_timestamp:\n",
        "            node_dict[target_node] = [len(node_dict), 'all']\n",
        "          else:\n",
        "            node_dict[target_node] = [len(node_dict), split]\n",
        "\n",
        "    return node_dict"
      ],
      "metadata": {
        "id": "tdK0xduhdF76"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_GraphAny_dataset(KG_data, node_dict, triplets):\n",
        "    node_features = []\n",
        "    labels = []\n",
        "    edges = []\n",
        "\n",
        "    train_mask = []\n",
        "    val_mask = []\n",
        "    test_mask = []\n",
        "\n",
        "    for key, value in node_dict.items():\n",
        "      # dalla chiave ottengo il nome del nodo e l'indice\n",
        "      entity_label, entity_index = modify_node_name(key)\n",
        "\n",
        "      # da KG_data prendo le features di quell'entità e la aggiungo a node_features\n",
        "      node_features.append(KG_data[entity_label].tf[entity_index])\n",
        "\n",
        "      # aggiugo la label di quell'entità a labels\n",
        "      labels.append(entity_label)\n",
        "\n",
        "      # in base al valore di split aggiorno le maschere\n",
        "      split_string = value[1]\n",
        "      if split_string == \"train\":\n",
        "        train_mask.append(True)\n",
        "        val_mask.append(False)\n",
        "        test_mask.append(False)\n",
        "      elif split_string == \"val\":\n",
        "        train_mask.append(False)\n",
        "        val_mask.append(True)\n",
        "        test_mask.append(False)\n",
        "      elif split_string == \"test\":\n",
        "        train_mask.append(False)\n",
        "        val_mask.append(False)\n",
        "        test_mask.append(True)\n",
        "      elif split_string == 'all':\n",
        "        train_mask.append(True)\n",
        "        val_mask.append(True)\n",
        "        test_mask.append(True)\n",
        "\n",
        "\n",
        "    # per ogni tripletta passata ricavo source e target\n",
        "    for triplet in triplets:\n",
        "      source_node = triplet[0]\n",
        "      edge_label = triplet[1]\n",
        "      target_node = triplet[2]\n",
        "\n",
        "      source_index = node_dict[source_node][0]\n",
        "      target_index = node_dict[target_node][0]\n",
        "\n",
        "      pair = [source_index, target_index]\n",
        "      if pair not in edges:\n",
        "        edges.append(pair)\n",
        "\n",
        "    return node_features, labels, edges, train_mask, val_mask, test_mask"
      ],
      "metadata": {
        "id": "CIDD9oHltRXX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F1 Dataset Creation"
      ],
      "metadata": {
        "id": "RgsNVRw6aQAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We download the f1-dataset\n",
        "f1_dataset = get_dataset(name=\"rel-f1\", download=True)\n",
        "\n",
        "# we download the entire database (also the test part)\n",
        "f1_db = f1_dataset.get_db(upto_test_timestamp = False)\n",
        "f1_col_to_stype_dict = get_stype_proposal(f1_db)\n",
        "\n",
        "# Generate graph data\n",
        "f1_data, f1_col_stats_dict = make_pkey_fkey_graph(\n",
        "    f1_db,\n",
        "    col_to_stype_dict = f1_col_to_stype_dict,  # Column types\n",
        "    text_embedder_cfg = text_embedder_cfg,  # Our chosen text encoder\n",
        "    cache_dir=os.path.join(\n",
        "        root_dir, f\"rel-f1_materialized_cache\"\n",
        "    ),  # Store materialized graph for convenience\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwaHZbQ-aVb2",
        "outputId": "3bf28b26-0ee2-4c42-a8d9-7b00efd5e38a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Database object from /root/.cache/relbench/rel-f1/db...\n",
            "Done in 0.19 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_val_timestep = f1_dataset.val_timestamp\n",
        "f1_test_timestep = f1_dataset.test_timestamp\n",
        "\n",
        "print(f\"The validation timestep is: {f1_val_timestep}\")\n",
        "print(f\"The test timestep is: {f1_test_timestep}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp7mUvgPaXvW",
        "outputId": "7f5b1eaa-3e98-41ae-9f34-9f88ffadfd50"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The validation timestep is: 2005-01-01 00:00:00\n",
            "The test timestep is: 2010-01-01 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA0OpBqsaYEC",
        "outputId": "19bf87e3-56bf-4485-b78f-a7db27e8559c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  drivers={ tf=TensorFrame([857, 6]) },\n",
              "  results={\n",
              "    tf=TensorFrame([26080, 11]),\n",
              "    time=[26080],\n",
              "  },\n",
              "  standings={\n",
              "    tf=TensorFrame([34124, 4]),\n",
              "    time=[34124],\n",
              "  },\n",
              "  constructor_results={\n",
              "    tf=TensorFrame([12290, 2]),\n",
              "    time=[12290],\n",
              "  },\n",
              "  constructors={ tf=TensorFrame([211, 3]) },\n",
              "  circuits={ tf=TensorFrame([77, 7]) },\n",
              "  qualifying={\n",
              "    tf=TensorFrame([9815, 3]),\n",
              "    time=[9815],\n",
              "  },\n",
              "  constructor_standings={\n",
              "    tf=TensorFrame([13051, 4]),\n",
              "    time=[13051],\n",
              "  },\n",
              "  races={\n",
              "    tf=TensorFrame([1101, 5]),\n",
              "    time=[1101],\n",
              "  },\n",
              "  (results, f2p_raceId, races)={ edge_index=[2, 26080] },\n",
              "  (races, rev_f2p_raceId, results)={ edge_index=[2, 26080] },\n",
              "  (results, f2p_driverId, drivers)={ edge_index=[2, 26080] },\n",
              "  (drivers, rev_f2p_driverId, results)={ edge_index=[2, 26080] },\n",
              "  (results, f2p_constructorId, constructors)={ edge_index=[2, 26080] },\n",
              "  (constructors, rev_f2p_constructorId, results)={ edge_index=[2, 26080] },\n",
              "  (standings, f2p_raceId, races)={ edge_index=[2, 34124] },\n",
              "  (races, rev_f2p_raceId, standings)={ edge_index=[2, 34124] },\n",
              "  (standings, f2p_driverId, drivers)={ edge_index=[2, 34124] },\n",
              "  (drivers, rev_f2p_driverId, standings)={ edge_index=[2, 34124] },\n",
              "  (constructor_results, f2p_raceId, races)={ edge_index=[2, 12290] },\n",
              "  (races, rev_f2p_raceId, constructor_results)={ edge_index=[2, 12290] },\n",
              "  (constructor_results, f2p_constructorId, constructors)={ edge_index=[2, 12290] },\n",
              "  (constructors, rev_f2p_constructorId, constructor_results)={ edge_index=[2, 12290] },\n",
              "  (qualifying, f2p_raceId, races)={ edge_index=[2, 9815] },\n",
              "  (races, rev_f2p_raceId, qualifying)={ edge_index=[2, 9815] },\n",
              "  (qualifying, f2p_driverId, drivers)={ edge_index=[2, 9815] },\n",
              "  (drivers, rev_f2p_driverId, qualifying)={ edge_index=[2, 9815] },\n",
              "  (qualifying, f2p_constructorId, constructors)={ edge_index=[2, 9815] },\n",
              "  (constructors, rev_f2p_constructorId, qualifying)={ edge_index=[2, 9815] },\n",
              "  (constructor_standings, f2p_raceId, races)={ edge_index=[2, 13051] },\n",
              "  (races, rev_f2p_raceId, constructor_standings)={ edge_index=[2, 13051] },\n",
              "  (constructor_standings, f2p_constructorId, constructors)={ edge_index=[2, 13051] },\n",
              "  (constructors, rev_f2p_constructorId, constructor_standings)={ edge_index=[2, 13051] },\n",
              "  (races, f2p_circuitId, circuits)={ edge_index=[2, 1101] },\n",
              "  (circuits, rev_f2p_circuitId, races)={ edge_index=[2, 1101] }\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node_names = ['standings', 'drivers', 'results', 'constructor_results', 'circuits', 'qualifying', 'races', 'constructors', 'constructor_standings']\n",
        "node_without_timestamp = ['drivers', 'circuits', 'constructors']\n",
        "edges_names = [('constructor_standings', 'f2p_raceId', 'races'),\n",
        "                ('races', 'rev_f2p_raceId', 'constructor_standings'),\n",
        "                ('constructor_standings', 'f2p_constructorId', 'constructors'),\n",
        "                ('constructors', 'rev_f2p_constructorId', 'constructor_standings'),\n",
        "                ('standings', 'f2p_raceId', 'races'),\n",
        "                ('races', 'rev_f2p_raceId', 'standings'),\n",
        "                ('standings', 'f2p_driverId', 'drivers'),\n",
        "                ('drivers', 'rev_f2p_driverId', 'standings'),\n",
        "                ('constructor_results', 'f2p_raceId', 'races'),\n",
        "                ('races', 'rev_f2p_raceId', 'constructor_results'),\n",
        "                ('constructor_results', 'f2p_constructorId', 'constructors'),\n",
        "                ('constructors', 'rev_f2p_constructorId', 'constructor_results'),\n",
        "                ('results', 'f2p_raceId', 'races'),\n",
        "                ('races', 'rev_f2p_raceId', 'results'),\n",
        "                ('results', 'f2p_driverId', 'drivers'),\n",
        "                ('drivers', 'rev_f2p_driverId', 'results'),\n",
        "                ('results', 'f2p_constructorId', 'constructors'),\n",
        "                ('constructors', 'rev_f2p_constructorId', 'results'),\n",
        "                ('qualifying', 'f2p_raceId', 'races'),\n",
        "                ('races', 'rev_f2p_raceId', 'qualifying'),\n",
        "                ('qualifying', 'f2p_driverId', 'drivers'),\n",
        "                ('drivers', 'rev_f2p_driverId', 'qualifying'),\n",
        "                ('qualifying', 'f2p_constructorId', 'constructors'),\n",
        "                ('constructors', 'rev_f2p_constructorId', 'qualifying'),\n",
        "                ('races', 'f2p_circuitId', 'circuits'),\n",
        "                ('circuits', 'rev_f2p_circuitId', 'races')]"
      ],
      "metadata": {
        "id": "VEEHxeJI1IT0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/train.txt\"\n",
        "val_path = \"https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/inference_valid.txt\"\n",
        "test_path = \"https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/inference_test.txt\""
      ],
      "metadata": {
        "id": "XwMGe1oRj22X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_triplets = read_triplets_from_file(train_path)\n",
        "val_triplets = read_triplets_from_file(val_path)\n",
        "test_triplets = read_triplets_from_file(test_path)"
      ],
      "metadata": {
        "id": "WqX7NI_oik58"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_dict = {}\n",
        "f1_dict = build_nodes_dictionary_from_triplets(node_dict = f1_dict,\n",
        "                                               triplets = train_triplets,\n",
        "                                               nodes_without_timestamp = node_without_timestamp,\n",
        "                                               split = 'train')\n",
        "f1_dict = build_nodes_dictionary_from_triplets(node_dict = f1_dict,\n",
        "                                               triplets = val_triplets,\n",
        "                                               nodes_without_timestamp = node_without_timestamp,\n",
        "                                               split = 'val')\n",
        "f1_dict = build_nodes_dictionary_from_triplets(node_dict = f1_dict,\n",
        "                                               triplets = test_triplets,\n",
        "                                               nodes_without_timestamp = node_without_timestamp,\n",
        "                                               split = 'test')"
      ],
      "metadata": {
        "id": "XxygBi4GidsF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_triplets = train_triplets + val_triplets + test_triplets"
      ],
      "metadata": {
        "id": "HQYeq38WtYLY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_node_features, f1_labels, f1_edges, f1_train_mask, f1_val_mask, f1_test_mask = build_GraphAny_dataset(KG_data = f1_data,\n",
        "                                                                                                          node_dict = f1_dict,\n",
        "                                                                                                          triplets = f1_triplets)"
      ],
      "metadata": {
        "id": "THozjGDKtXC1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_save = {\n",
        "    'node_features': f1_node_features,\n",
        "    'labels': f1_labels,\n",
        "    'edges': f1_edges,\n",
        "    'train_mask': f1_train_mask,\n",
        "    'val_mask': f1_val_mask,\n",
        "    'test_mask': f1_test_mask\n",
        "}\n",
        "\n",
        "with open('f1_data.pkl', 'wb') as f:\n",
        "    pickle.dump(data_to_save, f)"
      ],
      "metadata": {
        "id": "UMJ1WHR8Z5FP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PER LEGGERE DAL FILE\n",
        "\n",
        "with open('f1_data.pkl', 'rb') as f:\n",
        "    data_loaded = pickle.load(f)\n",
        "\n",
        "f1_node_features = data_loaded['node_features']\n",
        "f1_labels = data_loaded['labels']\n",
        "f1_edges = data_loaded['edges']\n",
        "f1_train_mask = data_loaded['train_mask']\n",
        "f1_val_mask = data_loaded['val_mask']\n",
        "f1_test_mask = data_loaded['test_mask']"
      ],
      "metadata": {
        "id": "FjXasOWL6Kle"
      },
      "execution_count": 20,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}