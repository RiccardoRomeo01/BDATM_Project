{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading dependencies"
      ],
      "metadata": {
        "id": "jUBnHeKnKQQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torch-scatter==2.1.2 torch-sparse==0.6.18 torch-geometric==2.4.0 -f https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
        "!pip install ninja easydict pyyaml\n",
        "!export CUDA_HOME=/usr/local/cuda-11.8/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zNCVFIlKTpz",
        "outputId": "995c468a-ddbf-415f-b9cc-69d3b0f70de8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.1.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m649.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2024.10.0)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
            "Installing collected packages: triton, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.1.0+cu118 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.1.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.1.0+cu118 triton-2.1.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
            "Collecting torch-scatter==2.1.2\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_scatter-2.1.2%2Bpt21cu118-cp311-cp311-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse==0.6.18\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_sparse-0.6.18%2Bpt21cu118-cp311-cp311-linux_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-geometric==2.4.0\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse==0.6.18) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (1.26.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (3.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (2.32.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (3.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (1.6.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (2024.12.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.4.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.4.0) (3.5.0)\n",
            "Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter, torch-sparse, torch-geometric\n",
            "Successfully installed torch-geometric-2.4.0 torch-scatter-2.1.2+pt21cu118 torch-sparse-0.6.18+pt21cu118\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.11/dist-packages (1.13)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading ULTRA"
      ],
      "metadata": {
        "id": "brzBGNlZKfEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/DeepGraphLearning/ULTRA.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOBw4--CKg_Q",
        "outputId": "73eb479f-b003-4d8a-ae51-c8b1ccc95bd8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ULTRA'...\n",
            "remote: Enumerating objects: 171, done.\u001b[K\n",
            "remote: Counting objects: 100% (79/79), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 171 (delta 61), reused 58 (delta 58), pack-reused 92 (from 1)\u001b[K\n",
            "Receiving objects: 100% (171/171), 7.62 MiB | 13.43 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Environment Setup"
      ],
      "metadata": {
        "id": "qe1eGvPLKk_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "sys.path.insert(0,'/content/ULTRA')\n",
        "from ultra.datasets import InductiveDataset, IngramInductive"
      ],
      "metadata": {
        "id": "sUlt6E_6KmsU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ULTRA Datasets Building"
      ],
      "metadata": {
        "id": "T4M71hD-KyYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we are going to build our Custom Datasets. In particular, our datasets are built similarly to the ones used for testing Ingram. Thus, we are going to extend the class IngramInductive.\n",
        "\n",
        "The data will be downloaded from the public git repository we previously built."
      ],
      "metadata": {
        "id": "J65llEQfK2FR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F1 Dataset"
      ],
      "metadata": {
        "id": "8_Qmo5DbLJRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code = \"\"\"\n",
        "class F1DatasetInductiveIngram(IngramInductive):\n",
        "    urls = [\n",
        "        \"https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-%s/inductive/train.txt\",\n",
        "        \"https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-%s/inductive/inference_graph.txt\",\n",
        "        \"https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-%s/inductive/inference_valid.txt\",\n",
        "        \"https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-%s/inductive/inference_test.txt\",\n",
        "    ]\n",
        "    name = \"f1_dataInductive\"\n",
        "\"\"\"\n",
        "\n",
        "# We write add the new class to the datasets file of ULTRA in order to easily run the entire script\n",
        "with open('/content/ULTRA/ultra/datasets.py', 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "if \"class F1DatasetInductiveIngram\" not in content:\n",
        "    with open('/content/ULTRA/ultra/datasets.py', 'a') as f:\n",
        "        f.write(code)"
      ],
      "metadata": {
        "id": "Z_bscPNmL4ia"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-Shot ULTRA Testing"
      ],
      "metadata": {
        "id": "gFdYnw4iMf7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we are going to test ULTRA on our custom datasets.\n",
        "\n",
        "As we know, we can use any of the checkpoints in the ckpts directory given by ULTRA, thus, we are going to test ULTRA on several checkpoints."
      ],
      "metadata": {
        "id": "ObjlGZrGMmwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing ULTRA on F1 Dataset"
      ],
      "metadata": {
        "id": "94dBiu19N7nY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using ULTRA trained on 3 graphs"
      ],
      "metadata": {
        "id": "-EIhtX10OC6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we are going to use the checkpoint: `ultra_3g.pth`, which has been trained on three graphs: `FB15k237`, `WN18RR`, `CoDExMedium` for 800,000 steps."
      ],
      "metadata": {
        "id": "9hsPSmSdOWZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### Here we setup the arguments used to run the script #####\n",
        "\n",
        "script_path = \"ULTRA/script/run.py\" # we want to run ULTRA on a single dataset\n",
        "config_path = \"ULTRA/config/inductive/inference.yaml\" # we want to test ULTRA on inference shot\n",
        "dataset = \"F1DatasetInductiveIngram\" # we want to use the F1 dataset\n",
        "version = \"v2\" # this is the version of input files\n",
        "epochs = 0 # we want to perform zero-shot, thus we impose zero training epochs\n",
        "bpe = \"null\" # batch-per-epochs\n",
        "gpus = \"[0]\" # we run the experimet on a single GPU\n",
        "checkpoint_path = \"/content/ULTRA/ckpts/ultra_3g.pth\" # we use ULTRA trained on 3 graphs"
      ],
      "metadata": {
        "id": "RfUX0ZLJO9bn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python {script_path} -c {config_path} --dataset {dataset} --version {version} --epochs {epochs} --bpe {bpe} --gpus {gpus} --ckpt {checkpoint_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9w9tP1Ah8NQ",
        "outputId": "b44faad7-d3fb-4040-fcdd-038cc9580210"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16:12:24   Random seed: 1024\n",
            "16:12:24   Config file: ULTRA/config/inductive/inference.yaml\n",
            "16:12:24   {'checkpoint': '/content/ULTRA/ckpts/ultra_3g.pth',\n",
            " 'dataset': {'class': 'F1DatasetInductiveIngram',\n",
            "             'root': '~/git/ULTRA/kg-datasets/',\n",
            "             'version': 'v2'},\n",
            " 'model': {'class': 'Ultra',\n",
            "           'entity_model': {'aggregate_func': 'sum',\n",
            "                            'class': 'EntityNBFNet',\n",
            "                            'hidden_dims': [64, 64, 64, 64, 64, 64],\n",
            "                            'input_dim': 64,\n",
            "                            'layer_norm': True,\n",
            "                            'message_func': 'distmult',\n",
            "                            'short_cut': True},\n",
            "           'relation_model': {'aggregate_func': 'sum',\n",
            "                              'class': 'RelNBFNet',\n",
            "                              'hidden_dims': [64, 64, 64, 64, 64, 64],\n",
            "                              'input_dim': 64,\n",
            "                              'layer_norm': True,\n",
            "                              'message_func': 'distmult',\n",
            "                              'short_cut': True}},\n",
            " 'optimizer': {'class': 'AdamW', 'lr': 0.0005},\n",
            " 'output_dir': '~/git/ULTRA/output',\n",
            " 'task': {'adversarial_temperature': 1,\n",
            "          'metric': ['mr', 'mrr', 'hits@1', 'hits@3', 'hits@10', 'hits@10_50'],\n",
            "          'name': 'InductiveInference',\n",
            "          'num_negative': 256,\n",
            "          'strict_negative': True},\n",
            " 'train': {'batch_per_epoch': None,\n",
            "           'batch_size': 16,\n",
            "           'gpus': [0],\n",
            "           'log_interval': 100,\n",
            "           'num_epoch': 0}}\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/train.txt\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/inference_graph.txt\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/inference_valid.txt\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/inference_test.txt\n",
            "Processing...\n",
            "/content/ULTRA/ultra/tasks.py:181: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
            "  Ahh = torch.sparse.mm(EhT, Eh).coalesce()\n",
            "Done!\n",
            "16:12:25   F1DatasetInductiveIngram(v2) dataset\n",
            "16:12:25   #train: 8671, #valid: 658, #test: 2033\n",
            "16:12:25   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:12:25   Evaluate on valid\n",
            "Load rspmm extension. This may take a while...\n",
            "16:14:12   mr: 2.33587\n",
            "16:14:12   mrr: 0.772117\n",
            "16:14:12   hits@1: 0.660334\n",
            "16:14:12   hits@3: 0.857143\n",
            "16:14:12   hits@10: 0.987082\n",
            "16:14:12   hits@10_50: 1\n",
            "16:14:12   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:14:12   Evaluate on test\n",
            "16:14:17   mr: 2.55263\n",
            "16:14:17   mrr: 0.764825\n",
            "16:14:17   hits@1: 0.650025\n",
            "16:14:17   hits@3: 0.859075\n",
            "16:14:17   hits@10: 0.981554\n",
            "16:14:17   hits@10_50: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using ULTRA trained on 4 graphs"
      ],
      "metadata": {
        "id": "ijISP5bZQ1I-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we are going to use the checkpoint: `ultra_4g.pth`, which has been trained on three graphs: `FB15k237`, `WN18RR`, `CoDExMedium`, `NELL995` for 400,000 steps."
      ],
      "metadata": {
        "id": "PcYyRwtHQ3jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### Here we setup the arguments used to run the script #####\n",
        "\n",
        "script_path = \"ULTRA/script/run.py\" # we want to run ULTRA on a single dataset\n",
        "config_path = \"ULTRA/config/inductive/inference.yaml\" # we want to test ULTRA on inference shot\n",
        "dataset = \"F1DatasetInductiveIngram\" # we want to use the F1 dataset\n",
        "version = \"v2\" # this is the version of input files\n",
        "epochs = 0 # we want to perform zero-shot, thus we impose zero training epochs\n",
        "bpe = \"null\" # batch-per-epochs\n",
        "gpus = \"[0]\" # we run the experimet on a single GPU\n",
        "checkpoint_path = \"/content/ULTRA/ckpts/ultra_4g.pth\" # we use ULTRA trained on 3 graphs"
      ],
      "metadata": {
        "id": "roG0vzaIQ_sx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python {script_path} -c {config_path} --dataset {dataset} --version {version} --epochs {epochs} --bpe {bpe} --gpus {gpus} --ckpt {checkpoint_path}"
      ],
      "metadata": {
        "id": "PRaOe1SyRDUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46936c9a-9ba4-4324-f8e1-8e5e7f60bcc1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16:24:50   Random seed: 1024\n",
            "16:24:50   Config file: ULTRA/config/inductive/inference.yaml\n",
            "16:24:50   {'checkpoint': '/content/ULTRA/ckpts/ultra_4g.pth',\n",
            " 'dataset': {'class': 'F1DatasetInductiveIngram',\n",
            "             'root': '~/git/ULTRA/kg-datasets/',\n",
            "             'version': 'v2'},\n",
            " 'model': {'class': 'Ultra',\n",
            "           'entity_model': {'aggregate_func': 'sum',\n",
            "                            'class': 'EntityNBFNet',\n",
            "                            'hidden_dims': [64, 64, 64, 64, 64, 64],\n",
            "                            'input_dim': 64,\n",
            "                            'layer_norm': True,\n",
            "                            'message_func': 'distmult',\n",
            "                            'short_cut': True},\n",
            "           'relation_model': {'aggregate_func': 'sum',\n",
            "                              'class': 'RelNBFNet',\n",
            "                              'hidden_dims': [64, 64, 64, 64, 64, 64],\n",
            "                              'input_dim': 64,\n",
            "                              'layer_norm': True,\n",
            "                              'message_func': 'distmult',\n",
            "                              'short_cut': True}},\n",
            " 'optimizer': {'class': 'AdamW', 'lr': 0.0005},\n",
            " 'output_dir': '~/git/ULTRA/output',\n",
            " 'task': {'adversarial_temperature': 1,\n",
            "          'metric': ['mr', 'mrr', 'hits@1', 'hits@3', 'hits@10', 'hits@10_50'],\n",
            "          'name': 'InductiveInference',\n",
            "          'num_negative': 256,\n",
            "          'strict_negative': True},\n",
            " 'train': {'batch_per_epoch': None,\n",
            "           'batch_size': 16,\n",
            "           'gpus': [0],\n",
            "           'log_interval': 100,\n",
            "           'num_epoch': 0}}\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/train.txt\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/inference_graph.txt\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/inference_valid.txt\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/inference_test.txt\n",
            "Processing...\n",
            "/content/ULTRA/ultra/tasks.py:181: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
            "  Ahh = torch.sparse.mm(EhT, Eh).coalesce()\n",
            "Done!\n",
            "16:24:51   F1DatasetInductiveIngram(v2) dataset\n",
            "16:24:51   #train: 8671, #valid: 658, #test: 2033\n",
            "16:24:52   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:24:52   Evaluate on valid\n",
            "Load rspmm extension. This may take a while...\n",
            "16:26:40   mr: 1.33967\n",
            "16:26:40   mrr: 0.901073\n",
            "16:26:40   hits@1: 0.846505\n",
            "16:26:40   hits@3: 0.943769\n",
            "16:26:40   hits@10: 1\n",
            "16:26:41   hits@10_50: 1\n",
            "16:26:41   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:26:41   Evaluate on test\n",
            "16:26:46   mr: 1.3003\n",
            "16:26:46   mrr: 0.908957\n",
            "16:26:46   hits@1: 0.855386\n",
            "16:26:46   hits@3: 0.957944\n",
            "16:26:46   hits@10: 1\n",
            "16:26:46   hits@10_50: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning ULTRA"
      ],
      "metadata": {
        "id": "s4BUJlHaSCCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing ULTRA on F1 Dataset"
      ],
      "metadata": {
        "id": "kwjS3iGATGxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using ULTRA trained on 3 graphs"
      ],
      "metadata": {
        "id": "zuTFCTzoTHV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we are going to use the checkpoint: `ultra_3g.pth`, which has been trained on three graphs: `FB15k237`, `WN18RR`, `CoDExMedium` for 800,000 steps."
      ],
      "metadata": {
        "id": "7zmqLfKcTO6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### Here we setup the arguments used to run the script #####\n",
        "\n",
        "script_path = \"ULTRA/script/run.py\" # we want to run ULTRA on a single dataset\n",
        "config_path = \"ULTRA/config/inductive/inference.yaml\" # we want to test ULTRA on inference shot\n",
        "dataset = \"F1DatasetInductiveIngram\" # we want to use the F1 dataset\n",
        "version = \"v2\" # this is the version of input files\n",
        "epochs = 5 # we want to perform pre-training, thus we impose a certain number of epochs\n",
        "bpe = 100 # batch-per-epochs\n",
        "gpus = \"[0]\" # we run the experimet on a single GPU\n",
        "checkpoint_path = \"/content/ULTRA/ckpts/ultra_3g.pth\" # we use ULTRA trained on 3 graphs"
      ],
      "metadata": {
        "id": "OQMlFLuLTTw0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python {script_path} -c {config_path} --dataset {dataset} --version {version} --epochs {epochs} --bpe {bpe} --gpus {gpus} --ckpt {checkpoint_path}"
      ],
      "metadata": {
        "id": "RzIPLmLdTn_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6917286-657f-4944-fdfe-658918fbc611"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16:30:36   Random seed: 1024\n",
            "16:30:36   Config file: ULTRA/config/inductive/inference.yaml\n",
            "16:30:36   {'checkpoint': '/content/ULTRA/ckpts/ultra_3g.pth',\n",
            " 'dataset': {'class': 'F1DatasetInductiveIngram',\n",
            "             'root': '~/git/ULTRA/kg-datasets/',\n",
            "             'version': 'v2'},\n",
            " 'model': {'class': 'Ultra',\n",
            "           'entity_model': {'aggregate_func': 'sum',\n",
            "                            'class': 'EntityNBFNet',\n",
            "                            'hidden_dims': [64, 64, 64, 64, 64, 64],\n",
            "                            'input_dim': 64,\n",
            "                            'layer_norm': True,\n",
            "                            'message_func': 'distmult',\n",
            "                            'short_cut': True},\n",
            "           'relation_model': {'aggregate_func': 'sum',\n",
            "                              'class': 'RelNBFNet',\n",
            "                              'hidden_dims': [64, 64, 64, 64, 64, 64],\n",
            "                              'input_dim': 64,\n",
            "                              'layer_norm': True,\n",
            "                              'message_func': 'distmult',\n",
            "                              'short_cut': True}},\n",
            " 'optimizer': {'class': 'AdamW', 'lr': 0.0005},\n",
            " 'output_dir': '~/git/ULTRA/output',\n",
            " 'task': {'adversarial_temperature': 1,\n",
            "          'metric': ['mr', 'mrr', 'hits@1', 'hits@3', 'hits@10', 'hits@10_50'],\n",
            "          'name': 'InductiveInference',\n",
            "          'num_negative': 256,\n",
            "          'strict_negative': True},\n",
            " 'train': {'batch_per_epoch': 100,\n",
            "           'batch_size': 16,\n",
            "           'gpus': [0],\n",
            "           'log_interval': 100,\n",
            "           'num_epoch': 5}}\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/train.txt\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/inference_graph.txt\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/inference_valid.txt\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/inference_test.txt\n",
            "Processing...\n",
            "/content/ULTRA/ultra/tasks.py:181: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
            "  Ahh = torch.sparse.mm(EhT, Eh).coalesce()\n",
            "Done!\n",
            "16:30:37   F1DatasetInductiveIngram(v2) dataset\n",
            "16:30:37   #train: 8671, #valid: 658, #test: 2033\n",
            "16:30:37   ------------------------------\n",
            "16:30:37   Number of parameters: 168705\n",
            "16:30:37   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:30:37   Epoch 0 begin\n",
            "Load rspmm extension. This may take a while...\n",
            "16:32:20   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:32:20   binary cross entropy: 1.61979\n",
            "16:32:29   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:32:29   Epoch 0 end\n",
            "16:32:29   ------------------------------\n",
            "16:32:29   average binary cross entropy: 0.496938\n",
            "16:32:29   Save checkpoint to model_epoch_1.pth\n",
            "16:32:29   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:32:29   Evaluate on valid\n",
            "16:32:31   mr: 4.34422\n",
            "16:32:31   mrr: 0.72392\n",
            "16:32:31   hits@1: 0.609423\n",
            "16:32:31   hits@3: 0.800912\n",
            "16:32:31   hits@10: 0.955927\n",
            "16:32:31   hits@10_50: 0.999994\n",
            "16:32:31   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:32:31   Epoch 1 begin\n",
            "16:32:31   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:32:31   binary cross entropy: 0.444981\n",
            "16:32:40   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:32:40   Epoch 1 end\n",
            "16:32:40   ------------------------------\n",
            "16:32:40   average binary cross entropy: 0.471581\n",
            "16:32:40   Save checkpoint to model_epoch_2.pth\n",
            "16:32:40   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:32:40   Evaluate on valid\n",
            "16:32:42   mr: 1.78267\n",
            "16:32:42   mrr: 0.812395\n",
            "16:32:42   hits@1: 0.721125\n",
            "16:32:42   hits@3: 0.882219\n",
            "16:32:42   hits@10: 0.989362\n",
            "16:32:42   hits@10_50: 1\n",
            "16:32:42   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:32:42   Epoch 2 begin\n",
            "16:32:42   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:32:42   binary cross entropy: 0.485796\n",
            "16:32:51   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:32:51   Epoch 2 end\n",
            "16:32:51   ------------------------------\n",
            "16:32:51   average binary cross entropy: 0.469782\n",
            "16:32:51   Save checkpoint to model_epoch_3.pth\n",
            "16:32:51   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:32:51   Evaluate on valid\n",
            "16:32:53   mr: 1.56535\n",
            "16:32:53   mrr: 0.84312\n",
            "16:32:53   hits@1: 0.762918\n",
            "16:32:53   hits@3: 0.905015\n",
            "16:32:53   hits@10: 1\n",
            "16:32:53   hits@10_50: 1\n",
            "16:32:53   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:32:53   Epoch 3 begin\n",
            "16:32:53   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:32:53   binary cross entropy: 0.3936\n",
            "16:33:02   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:33:02   Epoch 3 end\n",
            "16:33:02   ------------------------------\n",
            "16:33:02   average binary cross entropy: 0.476721\n",
            "16:33:02   Save checkpoint to model_epoch_4.pth\n",
            "16:33:02   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:33:02   Evaluate on valid\n",
            "16:33:04   mr: 1.79027\n",
            "16:33:04   mrr: 0.810513\n",
            "16:33:04   hits@1: 0.718085\n",
            "16:33:04   hits@3: 0.880699\n",
            "16:33:04   hits@10: 0.988602\n",
            "16:33:04   hits@10_50: 1\n",
            "16:33:04   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:33:04   Epoch 4 begin\n",
            "16:33:04   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:33:04   binary cross entropy: 0.421942\n",
            "16:33:13   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:33:13   Epoch 4 end\n",
            "16:33:13   ------------------------------\n",
            "16:33:13   average binary cross entropy: 0.478704\n",
            "16:33:13   Save checkpoint to model_epoch_5.pth\n",
            "16:33:13   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:33:13   Evaluate on valid\n",
            "16:33:15   mr: 1.56155\n",
            "16:33:15   mrr: 0.842754\n",
            "16:33:15   hits@1: 0.761398\n",
            "16:33:15   hits@3: 0.905015\n",
            "16:33:15   hits@10: 1\n",
            "16:33:15   hits@10_50: 1\n",
            "16:33:15   Load checkpoint from model_epoch_3.pth\n",
            "16:33:15   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:33:15   Evaluate on valid\n",
            "16:33:17   mr: 1.56535\n",
            "16:33:17   mrr: 0.84312\n",
            "16:33:17   hits@1: 0.762918\n",
            "16:33:17   hits@3: 0.905015\n",
            "16:33:17   hits@10: 1\n",
            "16:33:17   hits@10_50: 1\n",
            "16:33:17   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:33:17   Evaluate on test\n",
            "16:33:22   mr: 1.55386\n",
            "16:33:22   mrr: 0.841141\n",
            "16:33:22   hits@1: 0.757501\n",
            "16:33:22   hits@3: 0.911707\n",
            "16:33:22   hits@10: 1\n",
            "16:33:22   hits@10_50: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using ULTRA trained on 4 graphs"
      ],
      "metadata": {
        "id": "4t1H2R6BTI7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we are going to use the checkpoint: `ultra_4g.pth`, which has been trained on three graphs: `FB15k237`, `WN18RR`, `CoDExMedium`, `NELL995` for 400,000 steps."
      ],
      "metadata": {
        "id": "D1oM_A8OTPgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### Here we setup the arguments used to run the script #####\n",
        "\n",
        "script_path = \"ULTRA/script/run.py\" # we want to run ULTRA on a single dataset\n",
        "config_path = \"ULTRA/config/inductive/inference.yaml\" # we want to test ULTRA on inference shot\n",
        "dataset = \"F1DatasetInductiveIngram\" # we want to use the F1 dataset\n",
        "version = \"v2\" # this is the version of input files\n",
        "epochs = 5 # we want to perform pre-training, thus we impose a certain number of epochs\n",
        "bpe = 100 # batch-per-epochs\n",
        "gpus = \"[0]\" # we run the experimet on a single GPU\n",
        "checkpoint_path = \"/content/ULTRA/ckpts/ultra_4g.pth\" # we use ULTRA trained on 3 graphs"
      ],
      "metadata": {
        "id": "67UYUAyiTecM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python {script_path} -c {config_path} --dataset {dataset} --version {version} --epochs {epochs} --bpe {bpe} --gpus {gpus} --ckpt {checkpoint_path}"
      ],
      "metadata": {
        "id": "ZUPZQMHiTojD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6898914f-abfe-42f0-c699-74f33513e131"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16:37:16   Random seed: 1024\n",
            "16:37:16   Config file: ULTRA/config/inductive/inference.yaml\n",
            "16:37:16   {'checkpoint': '/content/ULTRA/ckpts/ultra_4g.pth',\n",
            " 'dataset': {'class': 'F1DatasetInductiveIngram',\n",
            "             'root': '~/git/ULTRA/kg-datasets/',\n",
            "             'version': 'v2'},\n",
            " 'model': {'class': 'Ultra',\n",
            "           'entity_model': {'aggregate_func': 'sum',\n",
            "                            'class': 'EntityNBFNet',\n",
            "                            'hidden_dims': [64, 64, 64, 64, 64, 64],\n",
            "                            'input_dim': 64,\n",
            "                            'layer_norm': True,\n",
            "                            'message_func': 'distmult',\n",
            "                            'short_cut': True},\n",
            "           'relation_model': {'aggregate_func': 'sum',\n",
            "                              'class': 'RelNBFNet',\n",
            "                              'hidden_dims': [64, 64, 64, 64, 64, 64],\n",
            "                              'input_dim': 64,\n",
            "                              'layer_norm': True,\n",
            "                              'message_func': 'distmult',\n",
            "                              'short_cut': True}},\n",
            " 'optimizer': {'class': 'AdamW', 'lr': 0.0005},\n",
            " 'output_dir': '~/git/ULTRA/output',\n",
            " 'task': {'adversarial_temperature': 1,\n",
            "          'metric': ['mr', 'mrr', 'hits@1', 'hits@3', 'hits@10', 'hits@10_50'],\n",
            "          'name': 'InductiveInference',\n",
            "          'num_negative': 256,\n",
            "          'strict_negative': True},\n",
            " 'train': {'batch_per_epoch': 100,\n",
            "           'batch_size': 16,\n",
            "           'gpus': [0],\n",
            "           'log_interval': 100,\n",
            "           'num_epoch': 5}}\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/train.txt\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/inference_graph.txt\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/inference_valid.txt\n",
            "Downloading https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/datasets/F1-v2/inductive/inference_test.txt\n",
            "Processing...\n",
            "/content/ULTRA/ultra/tasks.py:181: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
            "  Ahh = torch.sparse.mm(EhT, Eh).coalesce()\n",
            "Done!\n",
            "16:37:17   F1DatasetInductiveIngram(v2) dataset\n",
            "16:37:17   #train: 8671, #valid: 658, #test: 2033\n",
            "16:37:17   ------------------------------\n",
            "16:37:17   Number of parameters: 168705\n",
            "16:37:17   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:37:17   Epoch 0 begin\n",
            "Load rspmm extension. This may take a while...\n",
            "16:39:00   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:00   binary cross entropy: 0.901366\n",
            "16:39:09   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:09   Epoch 0 end\n",
            "16:39:09   ------------------------------\n",
            "16:39:09   average binary cross entropy: 0.49232\n",
            "16:39:09   Save checkpoint to model_epoch_1.pth\n",
            "16:39:09   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:09   Evaluate on valid\n",
            "16:39:11   mr: 1.27508\n",
            "16:39:11   mrr: 0.921418\n",
            "16:39:11   hits@1: 0.879179\n",
            "16:39:11   hits@3: 0.955927\n",
            "16:39:11   hits@10: 1\n",
            "16:39:11   hits@10_50: 1\n",
            "16:39:11   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:11   Epoch 1 begin\n",
            "16:39:11   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:11   binary cross entropy: 0.436403\n",
            "16:39:20   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:20   Epoch 1 end\n",
            "16:39:20   ------------------------------\n",
            "16:39:20   average binary cross entropy: 0.478298\n",
            "16:39:20   Save checkpoint to model_epoch_2.pth\n",
            "16:39:20   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:20   Evaluate on valid\n",
            "16:39:22   mr: 1.27508\n",
            "16:39:22   mrr: 0.921418\n",
            "16:39:22   hits@1: 0.879179\n",
            "16:39:22   hits@3: 0.955927\n",
            "16:39:22   hits@10: 1\n",
            "16:39:22   hits@10_50: 1\n",
            "16:39:22   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:22   Epoch 2 begin\n",
            "16:39:22   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:22   binary cross entropy: 0.517857\n",
            "16:39:31   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:31   Epoch 2 end\n",
            "16:39:31   ------------------------------\n",
            "16:39:31   average binary cross entropy: 0.47495\n",
            "16:39:31   Save checkpoint to model_epoch_3.pth\n",
            "16:39:31   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:31   Evaluate on valid\n",
            "16:39:33   mr: 1.27508\n",
            "16:39:33   mrr: 0.921418\n",
            "16:39:33   hits@1: 0.879179\n",
            "16:39:33   hits@3: 0.955927\n",
            "16:39:33   hits@10: 1\n",
            "16:39:33   hits@10_50: 1\n",
            "16:39:33   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:33   Epoch 3 begin\n",
            "16:39:33   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:33   binary cross entropy: 0.409378\n",
            "16:39:42   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:42   Epoch 3 end\n",
            "16:39:42   ------------------------------\n",
            "16:39:42   average binary cross entropy: 0.486624\n",
            "16:39:42   Save checkpoint to model_epoch_4.pth\n",
            "16:39:42   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:42   Evaluate on valid\n",
            "16:39:44   mr: 1.27508\n",
            "16:39:44   mrr: 0.921418\n",
            "16:39:44   hits@1: 0.879179\n",
            "16:39:44   hits@3: 0.955927\n",
            "16:39:44   hits@10: 1\n",
            "16:39:44   hits@10_50: 1\n",
            "16:39:44   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:44   Epoch 4 begin\n",
            "16:39:44   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:44   binary cross entropy: 0.432108\n",
            "16:39:53   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:53   Epoch 4 end\n",
            "16:39:53   ------------------------------\n",
            "16:39:53   average binary cross entropy: 0.484071\n",
            "16:39:53   Save checkpoint to model_epoch_5.pth\n",
            "16:39:53   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:53   Evaluate on valid\n",
            "16:39:55   mr: 1.27508\n",
            "16:39:55   mrr: 0.921418\n",
            "16:39:55   hits@1: 0.879179\n",
            "16:39:55   hits@3: 0.955927\n",
            "16:39:55   hits@10: 1\n",
            "16:39:55   hits@10_50: 1\n",
            "16:39:55   Load checkpoint from model_epoch_1.pth\n",
            "16:39:55   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:55   Evaluate on valid\n",
            "16:39:57   mr: 1.27508\n",
            "16:39:57   mrr: 0.921418\n",
            "16:39:57   hits@1: 0.879179\n",
            "16:39:57   hits@3: 0.955927\n",
            "16:39:57   hits@10: 1\n",
            "16:39:57   hits@10_50: 1\n",
            "16:39:57   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "16:39:57   Evaluate on test\n",
            "16:40:02   mr: 1.27127\n",
            "16:40:02   mrr: 0.922531\n",
            "16:40:02   hits@1: 0.882194\n",
            "16:40:02   hits@3: 0.958436\n",
            "16:40:02   hits@10: 1\n",
            "16:40:02   hits@10_50: 1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}