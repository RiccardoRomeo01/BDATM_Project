{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading GraphAny"
      ],
      "metadata": {
        "id": "UcNraNGefTp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/DeepGraphLearning/GraphAny.git"
      ],
      "metadata": {
        "id": "MC3-4ki8fWY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Environment Setup"
      ],
      "metadata": {
        "id": "LHVlit_sfoks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "import yaml\n",
        "\n",
        "sys.path.insert(0,'/content/GraphAny')\n",
        "\n",
        "!conda env create -f environment.yaml"
      ],
      "metadata": {
        "id": "qwHPwzzJfr1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F1 Dataset Implementation"
      ],
      "metadata": {
        "id": "7wg0WnaJhM3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update `configs/data.yaml`"
      ],
      "metadata": {
        "id": "sKbBdLDuhaLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# leggiamo tutto il file yaml\n",
        "with open('configs/data.yaml', 'r') as file:\n",
        "    data = yaml.safe_load(file)\n",
        "\n",
        "# aggiungo i metadati del dataset F1 al file yaml\n",
        "data['_ds_meta_data']['F1'] = 'relbench, f1_9_classes'\n",
        "\n",
        "\n",
        "with open('file.yaml', 'w') as file:\n",
        "    yaml.dump(data, file, default_flow_style=False, sort_keys=False)"
      ],
      "metadata": {
        "id": "VIuYTpWCjgcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('configs/data.yaml', 'r') as file:\n",
        "    data = yaml.safe_load(file)\n",
        "\n",
        "\n",
        "# Aggiungo il nuovo elemento _dataset_lookup\n",
        "data['_dataset_lookup'] = {\n",
        "    'F1Debug': {\n",
        "        'train': ['Wisconsin'],\n",
        "        'eval': ['F1']\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "with open('configs/data.yaml', 'w') as file:\n",
        "    yaml.dump(data, file, default_flow_style=False, sort_keys=False)"
      ],
      "metadata": {
        "id": "453-Ir-Okjtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the dataset interface and update `GraphDataset` class in `data.py`"
      ],
      "metadata": {
        "id": "kCjvqIMLmVON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code = \"\"\"\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import os.path\n",
        "import os.path as osp\n",
        "import re\n",
        "import ssl\n",
        "import sys\n",
        "import urllib\n",
        "\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from hydra.utils import instantiate\n",
        "from omegaconf import OmegaConf\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.manifold._utils import (\n",
        "    _binary_search_perplexity as sklearn_binary_search_perplexity,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from graphany.utils import logger, timer\n",
        "\n",
        "\n",
        "def get_entropy_normed_cond_gaussian_prob(X, entropy, metric=\"euclidean\"):\n",
        "    \"\"\"\n",
        "    #Parameters\n",
        "    #----------\n",
        "    #X:              The matrix for pairwise similarity\n",
        "    #entropy:     Perplexity of the conditional prob distribution\n",
        "    #Returns the entropy-normalized conditional gaussian probability based on distances.\n",
        "    #-------\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute pairwise distances\n",
        "    perplexity = np.exp2(entropy)\n",
        "    distances = pdist(X, metric=metric)\n",
        "    distances = squareform(distances)\n",
        "\n",
        "    # Compute the squared distances\n",
        "    distances **= 2\n",
        "    distances = distances.astype(np.float32)\n",
        "    return sklearn_binary_search_perplexity(distances, perplexity, verbose=0)\n",
        "\n",
        "\n",
        "def sample_k_nodes_per_label(label, visible_nodes, k, num_class):\n",
        "    ref_node_idx = [\n",
        "        (label[visible_nodes] == lbl).nonzero().view(-1) for lbl in range(num_class)\n",
        "    ]\n",
        "    sampled_indices = [\n",
        "        label_indices[torch.randperm(len(label_indices))[:k]]\n",
        "        for label_indices in ref_node_idx\n",
        "    ]\n",
        "    return visible_nodes[torch.cat(sampled_indices)]\n",
        "\n",
        "\n",
        "def get_data_split_masks(n_nodes, labels, num_train_nodes, label_idx=None, seed=42):\n",
        "    label_idx = np.arange(n_nodes)\n",
        "    test_rate_in_labeled_nodes = (len(labels) - num_train_nodes) / len(labels)\n",
        "    train_idx, test_and_valid_idx = train_test_split(\n",
        "        label_idx,\n",
        "        test_size=test_rate_in_labeled_nodes,\n",
        "        random_state=seed,\n",
        "        shuffle=True,\n",
        "        stratify=labels,\n",
        "    )\n",
        "    valid_idx, test_idx = train_test_split(\n",
        "        test_and_valid_idx,\n",
        "        test_size=0.5,\n",
        "        random_state=seed,\n",
        "        shuffle=True,\n",
        "        stratify=labels[test_and_valid_idx],\n",
        "    )\n",
        "    train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "    val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "\n",
        "    train_mask[train_idx] = True\n",
        "    val_mask[valid_idx] = True\n",
        "    test_mask[test_idx] = True\n",
        "\n",
        "    return train_mask, val_mask, test_mask\n",
        "\n",
        "\n",
        "def download_url(url: str, folder: str, log: bool = True, filename=None):\n",
        "    r\"\"\"#Modified from torch_geometric.data.download_url\n",
        "\n",
        "    #Downloads the content of an URL to a specific folder.\n",
        "\n",
        "    #Args:\n",
        "        #url (str): The URL.\n",
        "        #folder (str): The folder.\n",
        "        #log (bool, optional): If :obj:`False`, will not print anything to the\n",
        "            #console. (default: :obj:`True`)\n",
        "    \"\"\"\n",
        "\n",
        "    if filename is None:\n",
        "        filename = url.rpartition(\"/\")[2]\n",
        "        filename = filename if filename[0] == \"?\" else filename.split(\"?\")[0]\n",
        "\n",
        "    path = osp.join(folder, filename)\n",
        "\n",
        "    if osp.exists(path):  # pragma: no cover\n",
        "        if log and \"pytest\" not in sys.modules:\n",
        "            print(f\"Using existing file {filename}\", file=sys.stderr)\n",
        "        return path\n",
        "\n",
        "    if log and \"pytest\" not in sys.modules:\n",
        "        print(f\"Downloading {url}\", file=sys.stderr)\n",
        "\n",
        "    os.makedirs(osp.expanduser(osp.normpath(folder)), exist_ok=True)\n",
        "\n",
        "    context = ssl._create_unverified_context()\n",
        "    data = urllib.request.urlopen(url, context=context)\n",
        "\n",
        "    with open(path, \"wb\") as f:\n",
        "        # workaround for https://bugs.python.org/issue42853\n",
        "        while True:\n",
        "            chunk = data.read(10 * 1024 * 1024)\n",
        "            if not chunk:\n",
        "                break\n",
        "            f.write(chunk)\n",
        "\n",
        "    return path\n",
        "\n",
        "\n",
        "def load_relbench_dataset(url, raw_dir):\n",
        "    # Converts relbench dataset to DGL Graph format\n",
        "    download_path = download_url(url, raw_dir)\n",
        "    data = np.load(download_path)\n",
        "    node_features = torch.tensor(data['node_features'])\n",
        "    labels = torch.tensor(data['node_labels'])\n",
        "    edges = torch.tensor(data['edges'])\n",
        "\n",
        "    graph = dgl.graph((edges[:, 0], edges[:, 1]),\n",
        "                      num_nodes=len(node_features), idtype=torch.int32)\n",
        "    num_classes = len(labels.unique())\n",
        "    train_mask, val_mask, test_mask = torch.tensor(data['train_mask']), torch.tensor(data['val_mask']), torch.tensor(\n",
        "        data['test_mask'])\n",
        "\n",
        "    return graph, labels, num_classes, node_features, train_mask, val_mask, test_mask\n",
        "\n",
        "\n",
        "def load_heterophilous_dataset(url, raw_dir):\n",
        "    # Wrap Heterophilous to DGL Graph Dataset format https://arxiv.org/pdf/2302.11640.pdf\n",
        "    download_path = download_url(url, raw_dir)\n",
        "    data = np.load(download_path)\n",
        "    node_features = torch.tensor(data[\"node_features\"])\n",
        "    labels = torch.tensor(data[\"node_labels\"])\n",
        "    edges = torch.tensor(data[\"edges\"])\n",
        "\n",
        "    graph = dgl.graph(\n",
        "        (edges[:, 0], edges[:, 1]), num_nodes=len(node_features), idtype=torch.int\n",
        "    )\n",
        "    num_classes = len(labels.unique())\n",
        "    num_targets = 1 if num_classes == 2 else num_classes\n",
        "    if num_targets == 1:\n",
        "        labels = labels.float()\n",
        "    train_masks = torch.tensor(data[\"train_masks\"]).T\n",
        "    val_masks = torch.tensor(data[\"val_masks\"]).T\n",
        "    test_masks = torch.tensor(data[\"test_masks\"]).T\n",
        "\n",
        "    return graph, labels, num_classes, node_features, train_masks, val_masks, test_masks\n",
        "\n",
        "\n",
        "class CombinedDataset(pl.LightningDataModule):\n",
        "    def __init__(self, train_ds_dict, eval_ds_dict, cfg):\n",
        "        super().__init__()\n",
        "        self.train_ds_dict = train_ds_dict\n",
        "        self.eval_ds_dict = eval_ds_dict\n",
        "        self.all_ds = list(self.train_ds_dict.values()) + list(\n",
        "            self.eval_ds_dict.values()\n",
        "        )\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def to(self, device):\n",
        "        for ds in self.all_ds:\n",
        "            ds.to(device)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        sub_dataloaders = {\n",
        "            name: ds.train_dataloader() for name, ds in self.train_ds_dict.items()\n",
        "        }\n",
        "        return pl.utilities.combined_loader.CombinedLoader(sub_dataloaders, \"min_size\")\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        sub_dataloaders = {\n",
        "            name: ds.val_dataloader() for name, ds in self.eval_ds_dict.items()\n",
        "        }\n",
        "        # Use max_size instead of max_size_cycle to avoid repeated evaluation on small datasets\n",
        "        return pl.utilities.combined_loader.CombinedLoader(sub_dataloaders, \"max_size\")\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        sub_dataloaders = {\n",
        "            name: ds.test_dataloader() for name, ds in self.eval_ds_dict.items()\n",
        "        }\n",
        "        # Use max_size instead of max_size_cycle to avoid repeated evaluation on small datasets\n",
        "        return pl.utilities.combined_loader.CombinedLoader(sub_dataloaders, \"max_size\")\n",
        "\n",
        "\n",
        "class GraphDataset(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "            self,\n",
        "            cfg,\n",
        "            ds_name,\n",
        "            cache_dir,\n",
        "            train_batch_size=256,\n",
        "            val_test_batch_size=256,\n",
        "            n_hops=1,\n",
        "            preprocess_device=torch.device(\"cpu\"),\n",
        "            permute_label=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.name = ds_name\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.permute_label = permute_label  # For checking label equivariance\n",
        "        self.val_test_batch_size = val_test_batch_size\n",
        "        self.preprocess_device = preprocess_device\n",
        "\n",
        "        self.n_hops = n_hops\n",
        "\n",
        "        self.data_source, ds_alias = cfg[\"_ds_meta_data\"][ds_name].split(\", \")\n",
        "        self.gidtype = None\n",
        "        self.dist = None\n",
        "        self.unmasked_pred = None\n",
        "        if self.data_source == \"pyg\":\n",
        "            components = ds_alias.split(\".\")\n",
        "            ds_init_args = {\n",
        "                \"_target_\": f\"torch_geometric.datasets.{ds_alias}\",\n",
        "                \"root\": f\"{cfg.dirs.data_storage}{self.data_source}/{ds_alias}/\",\n",
        "            }\n",
        "            if len(components) == 2:  # If sub-dataset\n",
        "                ds_init_args[\"_target_\"] = f\"torch_geometric.datasets.{components[0]}\"\n",
        "                ds_init_args[\"name\"] = components[1]\n",
        "        elif self.data_source == \"dgl\":\n",
        "            ds_init_args = {\n",
        "                \"_target_\": f\"dgl.data.{ds_alias}\",\n",
        "                \"raw_dir\": f\"{cfg.dirs.data_storage}{self.data_source}/\",\n",
        "            }\n",
        "        elif self.data_source == \"ogb\":\n",
        "            ds_init_args = {\n",
        "                \"_target_\": f\"ogb.nodeproppred.DglNodePropPredDataset\",\n",
        "                \"root\": f\"{cfg.dirs.data_storage}{self.data_source}/\",\n",
        "                \"name\": ds_alias,\n",
        "            }\n",
        "        elif self.data_source == \"heterophilous\":\n",
        "            target = \"graphany.data.load_heterophilous_dataset\"\n",
        "            url = f\"https://raw.githubusercontent.com/yandex-research/heterophilous-graphs/main/data/{ds_alias}.npz\"\n",
        "            ds_init_args = {\n",
        "                \"_target_\": target,\n",
        "                \"raw_dir\": f\"{cfg.dirs.data_storage}{self.data_source}/\",\n",
        "                \"url\": url,\n",
        "            }\n",
        "        elif self.data_source == \"relbench\":\n",
        "            target = \"graphany.data.load_relbench_dataset\"\n",
        "            url = f\"https://raw.githubusercontent.com/RiccardoRomeo01/BDATM_project_public_data/main/GraphAny_datasets/{ds_alias}.pkl\"\n",
        "            ds_init_args = {\n",
        "                \"_target_\": target,\n",
        "                \"raw_dir\": f\"{cfg.dirs.data_storage}{self.data_source}/\",\n",
        "                \"url\": url,\n",
        "            }\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unsupported {self.data_source=}\")\n",
        "        self.data_init_args = OmegaConf.create(ds_init_args)\n",
        "        # self.cache_f_name = osp.join(\n",
        "        #     cache_dir, f'{self.name}_{n_hops}')\n",
        "        if cfg.get(\"feat_chn\"):\n",
        "            all_channels = \"+\".join([cfg.feat_chn, cfg.pred_chn])\n",
        "            all_hops = re.findall(r\"\\d+\", all_channels)\n",
        "            n_hops = max(max([int(_) for _ in all_hops]), n_hops)\n",
        "\n",
        "        self.split_index = 0\n",
        "        (\n",
        "            self.g,\n",
        "            self.label,\n",
        "            self.feat,\n",
        "            self.train_mask,\n",
        "            self.val_mask,\n",
        "            self.test_mask,\n",
        "            self.num_class,\n",
        "        ) = self.load_dataset(self.data_init_args)\n",
        "        self.n_nodes, self.n_edges = self.g.num_nodes(), self.g.num_edges()\n",
        "        self.cache_f_name = osp.join(\n",
        "            cache_dir,\n",
        "            f\"{self.name}_{n_hops}hop_selfloop={cfg.add_self_loop}_bidirected={cfg.to_bidirected}_split=\"\n",
        "            f\"{self.split_index}.pt\",\n",
        "        )\n",
        "\n",
        "        self.dist_f_name = osp.join(\n",
        "            cache_dir,\n",
        "            f\"{self.name}_{n_hops}hop_selfloop={cfg.add_self_loop}_bidirected={cfg.to_bidirected}_split=\"\n",
        "            f\"{self.split_index}_{cfg.feat_chn}_entropy={cfg.entropy}_dist.pt\",\n",
        "        )\n",
        "\n",
        "        self.gidtype = self.g.idtype\n",
        "        self.train_indices = self.train_mask.nonzero().view(-1)\n",
        "\n",
        "        (\n",
        "            self.features,\n",
        "            self.unmasked_pred,\n",
        "            self.dist,\n",
        "        ) = self.prepare_prop_features_logits_and_dist_features(\n",
        "            self.g, self.feat, n_hops=cfg.n_hops\n",
        "        )\n",
        "        # Remove the graph, as GraphAny doesn't use it in training\n",
        "        del self.g\n",
        "        del self.feat\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    def to(self, device):  # Supports nested dictionary\n",
        "        def to_device(input):\n",
        "            if input is None:\n",
        "                return None\n",
        "            elif isinstance(input, dict):\n",
        "                return {key: to_device(value) for key, value in input.items()}\n",
        "            elif isinstance(input, list):\n",
        "                return [to_device(item) for item in input]\n",
        "            elif hasattr(input, \"to\"):\n",
        "                return input.to(device)\n",
        "            else:\n",
        "                return (\n",
        "                    input  # Return as is if it's not a tensor or any nested structure\n",
        "                )\n",
        "\n",
        "        # Apply to_device to all attributes that may contain tensors\n",
        "        attrs = [\n",
        "            \"label\",\n",
        "            \"feat\",\n",
        "            \"train_mask\",\n",
        "            \"val_mask\",\n",
        "            \"test_mask\",\n",
        "            \"train_indices\",\n",
        "            \"unmasked_pred\",\n",
        "        ]\n",
        "        for attr in attrs:\n",
        "            if hasattr(self, attr):\n",
        "                setattr(self, attr, to_device(getattr(self, attr)))\n",
        "\n",
        "    def load_dataset(self, data_init_args):\n",
        "        dataset = instantiate(data_init_args)\n",
        "\n",
        "        if self.data_source == \"ogb\":\n",
        "            split_idx = dataset.get_idx_split()\n",
        "            train_indices, valid_indices, test_indices = (\n",
        "                split_idx[\"train\"],\n",
        "                split_idx[\"valid\"],\n",
        "                split_idx[\"test\"],\n",
        "            )\n",
        "            # graph: dgl graph object, label: torch tensor of shape (num_nodes, num_tasks)\n",
        "            g, label = dataset[0]\n",
        "            label = label.view(-1)\n",
        "\n",
        "            def to_mask(indices):\n",
        "                mask = torch.BoolTensor(g.number_of_nodes()).fill_(False)\n",
        "                mask[indices] = 1\n",
        "                return mask\n",
        "\n",
        "            train_mask, val_mask, test_mask = map(\n",
        "                to_mask, (train_indices, valid_indices, test_indices)\n",
        "            )\n",
        "\n",
        "            num_class = label.max().item() + 1\n",
        "\n",
        "            feat = g.ndata[\"feat\"]\n",
        "        elif self.data_source == \"heterophilous\":\n",
        "            g, label, num_class, feat, train_mask, val_mask, test_mask = dataset\n",
        "        elif self.data_source == \"relbench\":\n",
        "            g, label, num_class, feat, train_mask, val_mask, test_mask = dataset\n",
        "        elif self.data_source == \"dgl\":\n",
        "            g = dataset[0]\n",
        "            num_class = dataset.num_classes\n",
        "\n",
        "            # get node feature\n",
        "            feat = g.ndata[\"feat\"]\n",
        "\n",
        "            # get data split\n",
        "            train_mask = g.ndata[\"train_mask\"]\n",
        "            val_mask = g.ndata[\"val_mask\"]\n",
        "            test_mask = g.ndata[\"test_mask\"]\n",
        "\n",
        "            label = g.ndata[\"label\"]\n",
        "        elif self.data_source == \"pyg\":\n",
        "            g = dgl.graph((dataset.edge_index[0], dataset.edge_index[1]))\n",
        "            n_nodes = dataset.x.shape[0]\n",
        "            num_class = dataset.num_classes\n",
        "            # get node feature\n",
        "            feat = dataset.x\n",
        "            label = dataset.y\n",
        "\n",
        "            if (\n",
        "                    hasattr(dataset, \"train_mask\")\n",
        "                    and hasattr(dataset, \"val_mask\")\n",
        "                    and hasattr(dataset, \"test_mask\")\n",
        "            ):\n",
        "                train_mask, val_mask, test_mask = (\n",
        "                    dataset.train_mask,\n",
        "                    dataset.val_mask,\n",
        "                    dataset.test_mask,\n",
        "                )\n",
        "            else:\n",
        "                if label.ndim > 1:\n",
        "                    raise NotImplementedError(\n",
        "                        \"Multi-Label classification currently unsupported.\"\n",
        "                    )\n",
        "                logging.warning(\n",
        "                    f\"No dataset split found for {self.name}, splitting with semi-supervised settings!!\"\n",
        "                )\n",
        "                train_mask, val_mask, test_mask = get_data_split_masks(\n",
        "                    n_nodes, label, 20 * num_class, seed=self.cfg.seed\n",
        "                )\n",
        "\n",
        "                self.split_index = self.cfg.seed\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unsupported {self.data_source=}\")\n",
        "        if train_mask.ndim == 1:\n",
        "            pass  # only one train/val/test split\n",
        "        elif train_mask.ndim == 2:\n",
        "            # ! Multiple splits\n",
        "            # Modified: Use the ${seed} split if not specified!\n",
        "            split_index = self.data_init_args.get(\"split\", self.cfg.seed)\n",
        "            # Avoid invalid split index\n",
        "            self.split_index = split_index = (split_index % train_mask.ndim)\n",
        "            train_mask = train_mask[:, split_index].squeeze()\n",
        "            val_mask = val_mask[:, split_index].squeeze()\n",
        "            if test_mask.ndim == 2:\n",
        "                test_mask = test_mask[:, split_index].squeeze()\n",
        "        else:\n",
        "            raise ValueError(\"train/val/test masks have more than 2 dimensions\")\n",
        "        print(\n",
        "            f\"{self.name} {g.num_nodes()} {g.num_edges()} {feat.shape[1]} {num_class} {len(train_mask.nonzero())}\"\n",
        "        )\n",
        "\n",
        "        if self.cfg.add_self_loop:\n",
        "            g = dgl.add_self_loop(g)\n",
        "        else:\n",
        "            g = dgl.remove_self_loop(g)\n",
        "        if self.cfg.to_bidirected:\n",
        "            g = dgl.to_bidirected(g)\n",
        "        g = dgl.to_simple(g)  # Remove duplicate edges.\n",
        "        return g, label, feat, train_mask, val_mask, test_mask, num_class\n",
        "\n",
        "    def compute_linear_gnn_logits(\n",
        "            self, features, n_per_label_examples, visible_nodes, bootstrap=False\n",
        "    ):\n",
        "        # Compute and save LinearGNN logits into a dict. Note the computation is on CPU as torch does not support\n",
        "        # the gelss driver on GPU currently.\n",
        "        preds = {}\n",
        "        label, num_class, device = self.label, self.num_class, torch.device(\"cpu\")\n",
        "        label = label.to(device)\n",
        "        visible_nodes = visible_nodes.to(device)\n",
        "        for channel, F in features.items():\n",
        "            F = F.to(device)\n",
        "            if bootstrap:\n",
        "                ref_nodes = sample_k_nodes_per_label(\n",
        "                    label, visible_nodes, n_per_label_examples, num_class\n",
        "                )\n",
        "            else:\n",
        "                ref_nodes = visible_nodes\n",
        "            Y_L = torch.nn.functional.one_hot(label[ref_nodes], num_class).float()\n",
        "            with timer(\n",
        "                    f\"Solving with CPU driver (N={len(ref_nodes)}, d={F.shape[1]}, k={num_class})\",\n",
        "                    logger.debug,\n",
        "            ):\n",
        "                W = torch.linalg.lstsq(\n",
        "                    F[ref_nodes.cpu()].cpu(), Y_L.cpu(), driver=\"gelss\"\n",
        "                )[0]\n",
        "            preds[channel] = F @ W\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def compute_channel_logits(self, features, visible_nodes, sample, device):\n",
        "        pred_logits = self.compute_linear_gnn_logits(\n",
        "            {\n",
        "                c: features[c]\n",
        "                for c in set(self.cfg.feat_channels + self.cfg.pred_channels)\n",
        "            },\n",
        "            self.cfg.n_per_label_examples,\n",
        "            visible_nodes,\n",
        "            bootstrap=sample,\n",
        "        )\n",
        "        return {c: logits.to(device) for c, logits in pred_logits.items()}\n",
        "\n",
        "    def prepare_prop_features_logits_and_dist_features(self, g, input_feats, n_hops):\n",
        "        # Calculate Low-pass features containing AX, A^2X and High-pass features\n",
        "        # (I-A)X, and (I-A)^2X\n",
        "        if not os.path.exists(self.cache_f_name):\n",
        "            g = g.to(self.preprocess_device)\n",
        "            with timer(\n",
        "                    f\"Computing {self.name} message passing and normalized predictions to file {self.cache_f_name}\",\n",
        "                    logger.info,\n",
        "            ):\n",
        "                dim = input_feats.size(1)\n",
        "                LP = torch.zeros(n_hops, g.number_of_nodes(), dim).to(\n",
        "                    self.preprocess_device\n",
        "                )\n",
        "                HP = torch.zeros(n_hops, g.number_of_nodes(), dim).to(\n",
        "                    self.preprocess_device\n",
        "                )\n",
        "\n",
        "                g.ndata[\"LP\"] = input_feats.to(self.preprocess_device)\n",
        "                g.ndata[\"HP\"] = input_feats.to(self.preprocess_device)\n",
        "                for hop_idx in range(n_hops):\n",
        "                    # D^-1 A filter\n",
        "                    g.update_all(fn.copy_u(\"LP\", \"temp\"), fn.mean(\"temp\", \"LP\"))\n",
        "\n",
        "                    # (I - D^-1A) filter\n",
        "                    g.update_all(fn.copy_u(\"HP\", \"temp\"), fn.mean(\"temp\", \"HP_out\"))\n",
        "                    g.ndata[\"HP\"] = g.ndata[\"HP\"] - g.ndata[\"HP_out\"]\n",
        "\n",
        "                    LP[hop_idx] = g.ndata[\"LP\"].clone()\n",
        "                    HP[hop_idx] = g.ndata[\"HP\"].clone()\n",
        "                lp_feat_dict = {f\"L{l + 1}\": x for l, x in enumerate(LP)}\n",
        "                hp_feat_dict = {f\"H{l + 1}\": x for l, x in enumerate(HP)}\n",
        "\n",
        "                features = {\"X\": input_feats, **lp_feat_dict, **hp_feat_dict}\n",
        "                unmasked_pred = self.compute_channel_logits(\n",
        "                    features,\n",
        "                    self.train_indices,\n",
        "                    sample=False,\n",
        "                    device=self.preprocess_device,\n",
        "                )\n",
        "                torch.save((features, unmasked_pred), self.cache_f_name)\n",
        "        else:\n",
        "            features, unmasked_pred = torch.load(self.cache_f_name, map_location=\"cpu\")\n",
        "        if not os.path.exists(self.dist_f_name):\n",
        "            with timer(\n",
        "                    f\"Computing {self.name} conditional gaussian distances \"\n",
        "                    f\"and save to {self.dist_f_name}\",\n",
        "                    logger.info,\n",
        "            ):\n",
        "                # y_feat: n_nodes, n_channels, n_labels\n",
        "                y_feat = np.stack(\n",
        "                    [unmasked_pred[c].cpu().numpy() for c in self.cfg.feat_channels],\n",
        "                    axis=1,\n",
        "                )\n",
        "                # Conditional gaussian probability\n",
        "                bsz, n_channel, n_class = y_feat.shape\n",
        "                dist_feat_dim = n_channel * (n_channel - 1)\n",
        "                # Conditional gaussian probability\n",
        "                cond_gaussian_prob = np.zeros((bsz, n_channel, n_channel))\n",
        "                for i in range(bsz):\n",
        "                    cond_gaussian_prob[i, :, :] = get_entropy_normed_cond_gaussian_prob(\n",
        "                        y_feat[i, :, :], self.cfg.entropy\n",
        "                    )\n",
        "                dist = np.zeros((bsz, dist_feat_dim), dtype=np.float32)\n",
        "\n",
        "                # Compute pairwise distances between channels n_channels(n_channels-1)/2 total features\n",
        "                pair_index = 0\n",
        "                for c in range(n_channel):\n",
        "                    for c_prime in range(n_channel):\n",
        "                        if c != c_prime:  # Diagonal distances are useless\n",
        "                            dist[:, pair_index] = cond_gaussian_prob[:, c, c_prime]\n",
        "                            pair_index += 1\n",
        "\n",
        "                dist = torch.from_numpy(dist)\n",
        "                torch.save(dist, self.dist_f_name)\n",
        "        else:\n",
        "            dist = torch.load(self.dist_f_name, map_location=\"cpu\")\n",
        "        return features, unmasked_pred, dist\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_mask.nonzero().view(-1),\n",
        "            batch_size=self.train_batch_size,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_mask.nonzero().view(-1), batch_size=self.val_test_batch_size\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_mask.nonzero().view(-1), batch_size=self.val_test_batch_size\n",
        "        )\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "l5Y0yWWBowc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('utils/data.py', 'a') as file:\n",
        "    file.write(new_code)"
      ],
      "metadata": {
        "id": "riTRCN1OnR2k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}